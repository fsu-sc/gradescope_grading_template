# Default for entire file. Each question can override
# max_score: per assert
visibility: visible
max_score: 20
module: all_questions

# choices: instructor_file, student_file, yaml_file

# Folders are relative to the base project folder
#student_folder_name: student_github_template
student_folder_name: student_code_with_answers
instructor_folder_name: instructor_code_with_answers

i_answer_source: instructor_file
s_answer_source: student_file

# default fixtures for all my tests
# set to None for any questions that does not require fixtures
# All fixtures should be in a single file
fixtures: 
  # file that contains all the fixtures
  # Perhaps this should be part of the configuration file
  # Necessary for fixtures to work
  fixture:
    name: run_compute
    # Fixure arguments in a list
    args: [all_questions]

links: 
    - "https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/"
#----------------------------------------------------------------------------------------------------
questions:
- id: question1
  parts:
    - id: (a)
      type: float
      answer: 0.0288
      note: Calculate the probability. 

    - id: (b)
      type: float
      answer: 0.002
      note: Calculate the probability. 

    - id: (c)
      type: float
      answer: 0.008
      note: Calculate the probability. 

#----------------------------------------------------------------------------------------------------
- id: question2
  parts:
    - id: (a) A
      type: bool
      answer:  True
      explanation: > 
        Boosting reduces bias by focusing more on the instances that previous 
        models misclassified. This is achieved by assigning higher weights to 
        the misclassified instances, ensuring they are given more importance 
        in subsequent models.

    - id: (a) B
      type:  bool
      answer: False
      explanation: >
        Boosting does not employ parallel model training; it is an iterative 
        process where each model is trained sequentially, with each model 
        learning from the errors of its predecessors.

    - id: (a) C
      type:  bool
      answer: False
      explanation: >
        Boosting does not increase model diversity by training each model on 
        a random subset of the dataset. This description more accurately fits 
        bagging techniques, such as Random Forest, where each model is trained 
        independently on a random subset of the data. In contrast, boosting focuses 
        on instances that are harder to classify, adjusting weights after each iteration.

    - id: (a) D
      type: bool
      answer: True
      explanation: > 
        Boosting assigns weights to each training instance, adjusting these weights 
        after each round to prioritize harder-to-classify instances in subsequent models. 
        This dynamic weighting is a fundamental aspect of boosting, enabling it to 
        focus iteratively on the instances that previous models misclassified.

    - id: (b) A
      type: bool
      answer: True
      explanation: Boosting can significantly increase the model's variance due to the high weight it may place on outliers. In boosting, misclassified instances are given higher weights in subsequent rounds. If an outlier is consistently misclassified, it may receive an increasingly high weight, causing the model to overfit to that specific instance. This can lead to increased variance and reduced generalization performance on unseen data.

    - id: (b) B
      type: False
      answer: None
      explanation: Boosting does not often lead to underfitting. In fact, boosting is known for its ability to reduce bias and improve the model's performance on the training data. By focusing on misclassified instances and adjusting their weights, boosting aims to create a strong learner that can capture complex patterns in the data. Underfitting occurs when the model is too simple to capture the underlying patterns, which is not a typical characteristic of boosting.

    - id: (b) C
      type: bool
      answer: True
      explanation: The sequential nature of boosting can lead to increased training times compared to bagging. In boosting, each model is trained sequentially, with the weights of the instances being adjusted based on the performance of the previous models. This sequential process can be computationally expensive, especially when dealing with large datasets or complex models. In contrast, bagging can be parallelized since each model is trained independently on a bootstrap sample of the data.

    - id: (b) A
      type: bool
      answer: False
      explanation:  Boosting's focus on reducing bias does not necessarily make it less susceptible to overfitting compared to other ensemble methods. While boosting is effective at reducing bias, it can still overfit to the training data if not properly regularized. The high weights assigned to misclassified instances, especially outliers, can lead to overfitting. Techniques like early stopping, regularization, or using a validation set can help mitigate overfitting in boosting. Other ensemble methods, such as bagging or random forests, can be less prone to overfitting due to their randomization techniques.

    - id: (c) Weight update
      type: eval_float
      answer: '0.5*math.log((1-p)/p)'
      locals: {'p': [0.2, 0.4]}
      note: The formulas should only use the variable 'p'. The formulas should be a valid Python expression. Use the functions in the math module as required. 

    - id: (d) Weight influence
      type: float
      answer: 1.5275252316519468
      tol: 1.e-2
      note: the answer should be correct to 3 significant digits

#----------------------------------------------------------------------------------------------------
- id: question3
  parts:
    # "yes" or "no"
    - id: Agree?
      type:  string
      answer: "No"

    - id: Explain
      type:  explain_string
      answer: > 
        No, because the ensemble of base classifiers gets a better 
        classification performance only when at least some of the 
        base classifiers are better than a random classifier. In 
        this case, all the base classifiers (the coin tosses) are 
        random and therefore, their ensemble would not do any better 
        than any other random classifier. 


#----------------------------------------------------------------------------------------------------
- id: question4
  # type: default type for this question
  # choices: default for this question
  parts:
    - id: (a) e=0.5, independent
      type:  bool
      answer: False

    - id: (b), independent
      type:  bool
      answer: True

    - id: (c) identical
      type:  bool
      answer: False


#----------------------------------------------------------------------------------------------------
- id: question5
  parts:
    - id: (a)
      type: string
      answer: 'iii'
      choices: ['i', 'ii', 'iii', 'iv']

    - id: (b)
      type:  string
      answer: 'i'
      choices: ['i', 'ii', 'iii', 'iv']

    - id: (c)
      type:  string
      answer: 'ii'
      choices: ['i', 'ii', 'iii', 'iv']

    - id: (d)
      type:  string
      answer: 'iv'
      choices: ['i', 'ii', 'iii', 'iv']


#----------------------------------------------------------------------------------------------------
- id: question6
  parts:
    - id: (a) C1-TPR
      type: eval_float
      locals: {'p': [.1, .9]}
      answer: 'p'

    - id: (a) C2-TPR
      type: eval_float
      locals: {'p': [.1, .9]}
      answer: '2*p'

    - id: (a) C1-FPR
      type: eval_float
      locals: {'p': [.1, .9]}
      answer: 'p'

    - id: (a) C2-FPR
      type: eval_float
      locals: {'p': [.1, .9]}
      answer: '2*p'

    - id: (b) C2 better classifier than C1?
      type: string
      choices: ['yes', 'no']
      answer: no
      note: "Hint: The random guess line in an ROC curve corresponds to TPR=FPR."

    - id: (b) C2 better classifier than C1? Explain
      type: explain_string
      answer: >
        No, when p takes continuous value in the range [0, 1], both ROC 
        curves of C1 and C2 are the same with the random guess line, 
        which is TPR=FPR.

    - id: (c) Which metric? 
      type: string
      choices: ["TPR/FPR", "precision/recall"]
      answer: "TPR/FPR"

    - id: (c) explain
      type: explain_string
      answer: > 
        Since C1 and C2 are both random classifiers, expected precision 
        for both of them are always the same, so precision contains 
        no information here. Besides, the recall will make C2 a winner, 
        but recall evaluation is just comparing the random probabilities, 
        which is apparently meaningless. In practice, C1 and C2 are 
        equivalent because they are both randomly guessing, so using 
        {TPR and FPR} is more proper for their relative performance.

#----------------------------------------------------------------------------------------------------
- id: question7
  parts:
    # "C1 better than C2"
    # "C1 worse than C2"
    # "neither is better"
    - id: (i) Best classifier? 
      type: string
      choices: ["C1", "C2", "None"]
      answer: "None"

    - id: (i) Best classifier, explain
      type: explain_string
      answer: >
        None of C1 and C2 is better than the other.  The two classifiers 
        are random, as shown by their TPR/FPR (C1= 0.1/0.1; C2=0.5/0.5), 
        which lies on the random guess line of ROC curve. 

    - id: (ii) appropriate metric pair
      choices: ["TPR-FPR", "precision-recall-F1-Measure"]
      type: string
      answer: "TPR-FPR"

    - id: (ii) appropriate metric pair, explain
      type: explain_string
      answer: >
        Higher F-1 measure might be caused by C2 having a higher 
        probability of assigning an instance to (+) class, which 
        causes more instances to be classified as (+) (but does 
        not mean that those + predictions are accurate)

    - id: (iii) preferred classifier? 
      type: string
      choices: ["C1", "C2", "C3"]
      answer: "C3"

    - id: (iii) best classifier, explain 
      type: explain_string
      answer: > 
        C3 is the best classifier because it has the highest TPR/FPR=2. 
        TPR / FPR provides a better evaluation % of a model across 
        different population distribution. So, if C3 is better than C2 
        and C1 based on TPR / % FPR on this dataset, it will still perform 
        similarly when tested on different population with a different 
        distribution.


#----------------------------------------------------------------------------------------------------
- id: question8
  parts:
    - id: (a) precision for C0
      # keys: 'precision' and 'recall'
      type: eval_float
      locals: {"p": [0.01, 0.99]}
      answer: "1/10"

    - id: (a) recall for C0
      type: eval_float
      locals: {"p": [0.01, 0.99]}
      answer: "p"

    - id: (b) F-measure of C0
      type: eval_float
      locals: {"p": [0.01, 0.99]}
      answer: "(2*p/10) / (p + 1/10)"

    - id: C1 better than random?
      type: string
      choices: ["yes", "no", "unknown"]
      answer: "unknown"
      explanation: > 
        The F-measure cannot always be an appropriate measure for any classifier under any 
        scenario.  So, we can hardly say that C1 is better than a random classifier depending 
        only on the given information.

    - id: p-range
      type: float
      answer: 0.3
      tol: 1.e-3
      explanation: Simply equate F to p.
      note: > 
        What is the range of p for which C1 is better than random? 
        What is "?" in the expression "p > ?"


#----------------------------------------------------------------------------------------------------
- id: question9
  parts:
    - id: (i) metrics
      type: dict[string,float]
      answer: {'recall':0.53, 'precision':0.62, 'F-measure':0.57, 'accuracy':0.88}

    - id: (i) best metric? 
      type: string
      choices: ['recall', 'precision', 'F-measure', 'accuracy']
      answer: {'F-measure'}

    - id: (i) worst metric? 
      type: string
      choices: ['recall', 'precision', 'F-measure', 'accuracy']
      answer: {'accuracy'}

    - id: (ii) Explain your choices of best and worst metrics
      type: explain_string
      answer: >
        Accuracy is a poor metric because there is a class imbalance 
        problem (the sunshine (+) class is the minority), so accuracy 
        is not a good indicator in this situation while the 
        F-measure is an appropriate metric.

  
#----------------------------------------------------------------------------------------------------
- id: question10
  parts:
    - id: (a) better test based on F-measure? 
      type: string
      choices: ["T1", "T2"]
      answer: T1

    - id: (b) better test based on TPR/FPR?
      type: string
      choices: ["T1", "T2"]
      answer: T2

    - id: (c) Which evaluation measure to use between the two tests? 
      type: string
      choices: ["F1", "TPR/FPR"]
      answer: "TPR/FPR"

    - id: (c) Which evaluation measure? Explain
      type: explain_string
      answer:  >
        In this case, T2 is strictly better than T1 since both have the 
        same TPR and T2 has lower FPR.  Hence for any data set 
        (irrespective of skew), T2 will outperform T1 on any reasonable 
        measures.” Note that the value of F-measure depends on the skew of 
        the data that is being used for evaluation.  In particular, a given 
        classifier will have different F-scores on two different datasets 
        that have different ratio of positive and negative classes. Hence 
        F-score should be used to compare two classifiers only if they 
        are being tested on data sets with similar skew.  For this 
        specific question, if classifiers T1 and T2 were both tested 
        on the same data set (or on two data sets that have identical 
        skew), then T2 will have higher F-score than T1.  The problem 
        arises when we compare the F-score of T1 with F-score T2 but 
        these scores are computed on data sets with different skew.
        The value of TPR/FPR is independent of the skew in the data.  
        Still, it can’t be used as a measure of choice in all situations. 
        In fact, depending on the actual skew in the data, a classifier 
        C1 with better TPR/FPR than C2 can have worse F-score relative 
        to C2.   However, if both classifiers have identical TPR 
        (or FPR), then TPR/FPR can be used to select the best classifier 
        (i.e., classifier with higher TPR/FPR will be superior according 
        to any reasonable measure when the two classifiers are tested 
        on the exact same data set).

    - id: (d) Example scenario where you would reverse choise in (c)
      type: explain_string
      answer: >
        In a  scenario where you know the skew in the target population 
        matches that in the confusion matrix,  the F measure can be preferred 
        over {TPR/FPR}, as it captures both precision and recall for the target 
        population, and thus allows you to identify a test that does not 
        compromise one of these for the other.
